---
output: pdf_document
---
## Coursera Practical Machine Learning - Project
***Alejandro Fraga***
***June, 2016***

### Objective

One thing that people regularly these days is quantify how much of a particular activity they 
do, but they rarely quantify how well they do it. In this project, my goal will be to 
use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 
They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

### Preparation
Load the data from the indivuals who participated in the study and the libraries needed to perform the analysis: 

```{r echo=FALSE}
library(knitr)
library(caret)
library(randomForest)
library(ggplot2)
library(dplyr)


if (!file.exists("pml-training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
}
if (!file.exists("pml-testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
}

test  <- read.csv("pml-testing.csv", sep = ",", na.strings = c("", "NA"))
train_data <- read.csv("pml-training.csv", sep = ",", na.strings = c("", "NA"))

```
Next we need to do some data preparation

```{r}
# Columns full of NAs are not useful, let's clean them
relevantFeatures <- names(test[,colSums(is.na(test)) == 0])[8:59]
# Only use relevant features used in the test_data cases.
train_data <- train_data[,c(relevantFeatures,"classe")]
test <- test[,c(relevantFeatures,"problem_id")]

```

### Boostrap the training set

I will hold 25% of the data set for testing 

```{r}
set.seed(246)
inTrain = createDataPartition(train_data$classe, p = 0.75, list = F)
training = train_data[inTrain,]
testing = train_data[-inTrain,]
```

### Feature Identification
To simplify the analysis, let's remove those features from the training set which are highly correlated (>90%)

```{r}
outcome = which(names(training) == "classe")
highCorrCols = findCorrelation(abs(cor(training[,-outcome])),0.90)
# highCorrFeatures variable will subset those highly correlated features
highCorrFeatures = names(training)[highCorrCols]
training = training[,-highCorrCols]
outcome = which(names(training) == "classe")
str(outcome)
```
From this analysis I found that those features with high correlation are: 
accel_belt_z, roll_belt, accel_belt_y, accel_belt_x, gyros_arm_y, gyros_forearm_z, and gyros_dumbbell_x.

### Random Forest
As we learn the Random Forest method is good for non linear features as is the case on this stufy plus reduces overfitting. First I will use this method to discover the most important features.

```{r}
featuresRF = randomForest(training[,-outcome], training[,outcome], importance = T)
importanceRF = data.frame(featuresRF$importance)
impFeatures = order(-importanceRF$MeanDecreaseGini)
inImp = createDataPartition(train_data$classe, p = 0.05, list = F)
```
The feature plot for the 4 most importan features (pitch_belt, yaw_belt, total_accel_belt, gyros_belt_x) is shown below:
```{r}
featurePlot(training[inImp,impFeatures[1:4]],training$classe[inImp], plot = "pairs")
```

### Training our dataset
Next I will train the model using random forest and k-nearest neighbors comparison
```{r}
# Developing KNN model
ctrlKNN = trainControl(method = "adaptive_cv")
modelKNN = train(classe ~ ., training, method = "knn", trControl = ctrlKNN)
KNNPredTrain <- predict(modelKNN, newdata=training)
KNNAccuracyTrain <- confusionMatrix(KNNPredTrain, training$classe)
KNNAccuracyTrain


# Working on Random Forest Model
ctrlRF = trainControl(method = "oob")
#modelRF = train(classe ~ ., training, method = "rf", ntree = 50, trControl = ctrlRF)
modelRF <- randomForest(classe ~ ., data=training)
#rFinMod <- modelRF$finalModel
RFPredTrain <- predict(modelRF, newdata=training, type="class")
RFAccuracyTrain <- confusionMatrix(RFPredTrain, training$classe)
RFAccuracyTrain
```

### Testing the initial model
As we can see the the random forest provides better accuracy compared with k-nearest neighbors method. Next I provide the confusion matrix between the two models applied to the training set

```{r}
resultsKNN = data.frame(modelKNN$results)
resultsRF = data.frame(modelRF$results)
resultsRF
```

### Comparing Results with Test set

```{r}
PredTest <- predict(modelRF, testing)
AccuracyTest <- confusionMatrix(PredTest, testing$classe)
AccuracyTest
```


### Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity 
Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in 
Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4BnUwkLsS
